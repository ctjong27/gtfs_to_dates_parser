{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages: 6\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://transitfeeds.com/p/mta/79\"\n",
    "response = requests.get(url)\n",
    "total_pages = 0\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    pagination = soup.find(\"ul\", class_=\"pagination\")\n",
    "    last_page_link = pagination.find_all(\"a\")[-1] # second to last link\n",
    "    total_pages = int(last_page_link.text)\n",
    "    print(f\"Total number of pages: {total_pages}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221129.zip already exists.\n",
      "20221129 folder already exists.\n",
      "20221121.zip already exists.\n",
      "20221121 folder already exists.\n",
      "20220615.zip already exists.\n",
      "20220615 folder already exists.\n",
      "20211210.zip already exists.\n",
      "20211210 folder already exists.\n",
      "20211109.zip already exists.\n",
      "20211109 folder already exists.\n",
      "20210713.zip already exists.\n",
      "20210713 folder already exists.\n",
      "20210615.zip already exists.\n",
      "20210615 folder already exists.\n",
      "20210503.zip already exists.\n",
      "20210503 folder already exists.\n",
      "20210315.zip already exists.\n",
      "20210315 folder already exists.\n",
      "20201102.zip already exists.\n",
      "20201102 folder already exists.\n",
      "20201001.zip already exists.\n",
      "20201001 folder already exists.\n",
      "20200910.zip already exists.\n",
      "20200910 folder already exists.\n",
      "20200814.zip already exists.\n",
      "20200814 folder already exists.\n",
      "20200430.zip already exists.\n",
      "20200430 folder already exists.\n",
      "20200109.zip already exists.\n",
      "20200109 folder already exists.\n",
      "20191231.zip already exists.\n",
      "20191231 folder already exists.\n",
      "20191112.zip already exists.\n",
      "20191112 folder already exists.\n",
      "20191003.zip already exists.\n",
      "20191003 folder already exists.\n",
      "20190909.zip already exists.\n",
      "20190909 folder already exists.\n",
      "20190509.zip already exists.\n",
      "20190509 folder already exists.\n",
      "20190423.zip already exists.\n",
      "20190423 folder already exists.\n",
      "20190420.zip already exists.\n",
      "20190420 folder already exists.\n",
      "20181221.zip already exists.\n",
      "20181221 folder already exists.\n",
      "20181113.zip already exists.\n",
      "20181113 folder already exists.\n",
      "20181004.zip already exists.\n",
      "20181004 folder already exists.\n",
      "20180908.zip already exists.\n",
      "20180908 folder already exists.\n",
      "20180708.zip already exists.\n",
      "20180708 folder already exists.\n",
      "20180622.zip already exists.\n",
      "20180622 folder already exists.\n",
      "20180615.zip already exists.\n",
      "20180615 folder already exists.\n",
      "20180109.zip already exists.\n",
      "20180109 folder already exists.\n",
      "20171109.zip already exists.\n",
      "20171109 folder already exists.\n",
      "20171106.zip already exists.\n",
      "20171106 folder already exists.\n",
      "20170919.zip already exists.\n",
      "20170919 folder already exists.\n",
      "20170712.zip already exists.\n",
      "20170712 folder already exists.\n",
      "20170619.zip already exists.\n",
      "20170619 folder already exists.\n",
      "20170130.zip already exists.\n",
      "20170130 folder already exists.\n",
      "20170124.zip already exists.\n",
      "20170124 folder already exists.\n",
      "20170113.zip already exists.\n",
      "20170113 folder already exists.\n",
      "20161222.zip already exists.\n",
      "20161222 folder already exists.\n",
      "20161210.zip already exists.\n",
      "20161210 folder already exists.\n",
      "20161107.zip already exists.\n",
      "20161107 folder already exists.\n",
      "20161103.zip already exists.\n",
      "20161103 folder already exists.\n",
      "20160627.zip already exists.\n",
      "20160627 folder already exists.\n",
      "20160502.zip already exists.\n",
      "20160502 folder already exists.\n",
      "20151207.zip already exists.\n",
      "20151207 folder already exists.\n",
      "20150914.zip already exists.\n",
      "20150914 folder already exists.\n",
      "20150909.zip already exists.\n",
      "20150909 folder already exists.\n",
      "20150901.zip already exists.\n",
      "20150901 folder already exists.\n",
      "20150616.zip already exists.\n",
      "20150616 folder already exists.\n",
      "20141204.zip already exists.\n",
      "20141204 folder already exists.\n",
      "20140919.zip already exists.\n",
      "20140919 folder already exists.\n",
      "20140822.zip already exists.\n",
      "20140822 folder already exists.\n",
      "20140801.zip already exists.\n",
      "20140801 folder already exists.\n",
      "20140715.zip already exists.\n",
      "20140715 folder already exists.\n",
      "20140626.zip already exists.\n",
      "20140626 folder already exists.\n",
      "20140326.zip already exists.\n",
      "20140326 folder already exists.\n",
      "20140204.zip already exists.\n",
      "20140204 folder already exists.\n",
      "20131030.zip already exists.\n",
      "20131030 folder already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "a_text_list = []\n",
    "base_url = 'https://transitfeeds.com/'\n",
    "\n",
    "# Loop through pages 1 to total_pages\n",
    "for page_num in range(1, total_pages + 1):\n",
    "    url = f'https://transitfeeds.com/p/mta/79?p={page_num}'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the first tbody element\n",
    "    tbody_element = soup.find('tbody')\n",
    "\n",
    "    # Find all tr elements within the current tbody\n",
    "    tr_elements = tbody_element.find_all('tr')\n",
    "\n",
    "    # Loop through each tr element\n",
    "    for tr in tr_elements:\n",
    "        # Find the first td element within the current tr\n",
    "        td_element = tr.find('td')\n",
    "\n",
    "        # Find the first a element within the current td and extract its text\n",
    "        a_element = td_element.find('a')\n",
    "        a_text = a_element.text\n",
    "\n",
    "        # Convert the date string to the desired format\n",
    "        date_obj = datetime.strptime(a_text, '%d %B %Y')\n",
    "        formatted_date = date_obj.strftime('%Y%m%d')\n",
    "        a_text_list.append(formatted_date)\n",
    "\n",
    "        # Find the second td element within the current tr\n",
    "        td_element_2 = tr.find_all('td')[3]\n",
    "\n",
    "        # Find the second a element within the current td and extract its href\n",
    "        a_element_2 = td_element_2.find_all('a')[1]\n",
    "        a_href = a_element_2.get('href')\n",
    "\n",
    "        # Check if the files/zip_files and files/extracted directories exist\n",
    "        if not os.path.exists('files/zip_files'):\n",
    "            os.makedirs('files/zip_files')\n",
    "        if not os.path.exists('files/extracted'):\n",
    "            os.makedirs('files/extracted')\n",
    "\n",
    "        # Download the zip file and save it to files/zip_files\n",
    "        zip_file_path = f'files/zip_files/{formatted_date}.zip'\n",
    "        if os.path.exists(zip_file_path):\n",
    "            print(f'{formatted_date}.zip already exists.')\n",
    "        else:\n",
    "            # print(f'Downloading {formatted_date}.zip...')\n",
    "            url = f'{base_url}{a_href}'\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(zip_file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f'{formatted_date}.zip downloaded successfully.')\n",
    "            else:\n",
    "                print(f'Error downloading {formatted_date}.zip. Status code: {response.status_code}')\n",
    "\n",
    "        # Extract the zip file to files/extracted\n",
    "        extracted_folder_path = f'files/extracted/{formatted_date}'\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            print(f'{formatted_date} folder already exists.')\n",
    "        else:\n",
    "            # print(f'Extracting {formatted_date}.zip...')\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extracted_folder_path)\n",
    "            print(f'{formatted_date}.zip extracted successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20221129', '20221121', '20220615', '20211210', '20211109', '20210713', '20210615', '20210503', '20210315', '20201102', '20201001', '20200910', '20200814', '20200430', '20200109', '20191231', '20191112', '20191003', '20190909', '20190509', '20190423', '20190420', '20181221', '20181113', '20181004', '20180908', '20180708', '20180622', '20180615', '20180109', '20171109', '20171106', '20170919', '20170712', '20170619', '20170130', '20170124', '20170113', '20161222', '20161210', '20161107', '20161103', '20160627', '20160502', '20151207', '20150914', '20150909', '20150901', '20150616', '20141204', '20140919', '20140822', '20140801', '20140715', '20140626', '20140326', '20140204', '20131030']\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# a_text_list = []\n",
    "\n",
    "# # Loop through pages 1 to total_pages\n",
    "# for page_num in range(1, total_pages + 1):\n",
    "#     url = f'https://transitfeeds.com/p/mta/79?p={page_num}'\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     # Find the first tbody element\n",
    "#     tbody_element = soup.find('tbody')\n",
    "\n",
    "#     # Find all tr elements within the current tbody\n",
    "#     tr_elements = tbody_element.find_all('tr')\n",
    "\n",
    "#     # Loop through each tr element\n",
    "#     for tr in tr_elements:\n",
    "#         # Find the first td element within the current tr\n",
    "#         td_element = tr.find('td')\n",
    "\n",
    "#         # Find the first a element within the current td and extract its text\n",
    "#         a_element = td_element.find('a')\n",
    "#         a_text = a_element.text\n",
    "\n",
    "#         # Convert the date string to the desired format\n",
    "#         date_obj = datetime.strptime(a_text, '%d %B %Y')\n",
    "#         formatted_date = date_obj.strftime('%Y%m%d')\n",
    "#         a_text_list.append(formatted_date)\n",
    "\n",
    "# print(a_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/zip_files/20221129.zip already exists\n",
      "files/extracted/20221129 already exists\n",
      "files/zip_files/20221121.zip already exists\n",
      "files/extracted/20221121 already exists\n",
      "files/zip_files/20220615.zip already exists\n",
      "files/extracted/20220615 already exists\n",
      "files/zip_files/20211210.zip already exists\n",
      "files/extracted/20211210 already exists\n",
      "files/zip_files/20211109.zip already exists\n",
      "files/extracted/20211109 already exists\n",
      "files/zip_files/20210713.zip already exists\n",
      "files/extracted/20210713 already exists\n",
      "files/zip_files/20210615.zip already exists\n",
      "files/extracted/20210615 already exists\n",
      "files/zip_files/20210503.zip already exists\n",
      "files/extracted/20210503 already exists\n",
      "files/zip_files/20210315.zip already exists\n",
      "files/extracted/20210315 already exists\n",
      "files/zip_files/20201102.zip already exists\n",
      "files/extracted/20201102 already exists\n",
      "files/zip_files/20201001.zip already exists\n",
      "files/extracted/20201001 already exists\n",
      "files/zip_files/20200910.zip already exists\n",
      "files/extracted/20200910 already exists\n",
      "files/zip_files/20200814.zip already exists\n",
      "files/extracted/20200814 already exists\n",
      "files/zip_files/20200430.zip already exists\n",
      "files/extracted/20200430 already exists\n",
      "files/zip_files/20200109.zip already exists\n",
      "files/extracted/20200109 already exists\n",
      "files/zip_files/20191231.zip already exists\n",
      "files/extracted/20191231 already exists\n",
      "files/zip_files/20191112.zip already exists\n",
      "files/extracted/20191112 already exists\n",
      "files/zip_files/20191003.zip already exists\n",
      "files/extracted/20191003 already exists\n",
      "files/zip_files/20190909.zip already exists\n",
      "files/extracted/20190909 already exists\n",
      "files/zip_files/20190509.zip already exists\n",
      "files/extracted/20190509 already exists\n",
      "files/zip_files/20190423.zip already exists\n",
      "files/extracted/20190423 already exists\n",
      "files/zip_files/20190420.zip already exists\n",
      "files/extracted/20190420 already exists\n",
      "files/zip_files/20181221.zip already exists\n",
      "files/extracted/20181221 already exists\n",
      "files/zip_files/20181113.zip already exists\n",
      "files/extracted/20181113 already exists\n",
      "files/zip_files/20181004.zip already exists\n",
      "files/extracted/20181004 already exists\n",
      "files/zip_files/20180908.zip already exists\n",
      "files/extracted/20180908 already exists\n",
      "files/zip_files/20180708.zip already exists\n",
      "files/extracted/20180708 already exists\n",
      "files/zip_files/20180622.zip already exists\n",
      "files/extracted/20180622 already exists\n",
      "files/zip_files/20180615.zip already exists\n",
      "files/extracted/20180615 already exists\n",
      "files/zip_files/20180109.zip already exists\n",
      "files/extracted/20180109 already exists\n",
      "files/zip_files/20171109.zip already exists\n",
      "files/extracted/20171109 already exists\n",
      "files/zip_files/20171106.zip already exists\n",
      "files/extracted/20171106 already exists\n",
      "files/zip_files/20170919.zip already exists\n",
      "files/extracted/20170919 already exists\n",
      "files/zip_files/20170712.zip already exists\n",
      "files/extracted/20170712 already exists\n",
      "files/zip_files/20170619.zip already exists\n",
      "files/extracted/20170619 already exists\n",
      "files/zip_files/20170130.zip already exists\n",
      "files/extracted/20170130 already exists\n",
      "files/zip_files/20170124.zip already exists\n",
      "files/extracted/20170124 already exists\n",
      "files/zip_files/20170113.zip already exists\n",
      "files/extracted/20170113 already exists\n",
      "files/zip_files/20161222.zip already exists\n",
      "files/extracted/20161222 already exists\n",
      "files/zip_files/20161210.zip already exists\n",
      "files/extracted/20161210 already exists\n",
      "files/zip_files/20161107.zip already exists\n",
      "files/extracted/20161107 already exists\n",
      "files/zip_files/20161103.zip already exists\n",
      "files/extracted/20161103 already exists\n",
      "files/zip_files/20160627.zip already exists\n",
      "files/extracted/20160627 already exists\n",
      "files/zip_files/20160502.zip already exists\n",
      "files/extracted/20160502 already exists\n",
      "files/zip_files/20151207.zip already exists\n",
      "files/extracted/20151207 already exists\n",
      "files/zip_files/20150914.zip already exists\n",
      "files/extracted/20150914 already exists\n",
      "files/zip_files/20150909.zip already exists\n",
      "files/extracted/20150909 already exists\n",
      "files/zip_files/20150901.zip already exists\n",
      "files/extracted/20150901 already exists\n",
      "files/zip_files/20150616.zip already exists\n",
      "files/extracted/20150616 already exists\n",
      "Error downloading data for 20141204\n",
      "Error downloading data for 20140919\n",
      "Error downloading data for 20140822\n",
      "Error downloading data for 20140801\n",
      "Error downloading data for 20140715\n",
      "Error downloading data for 20140626\n",
      "Error downloading data for 20140326\n",
      "Error downloading data for 20140204\n",
      "Error downloading data for 20131030\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# # Define the base URL\n",
    "# base_url = 'https://transitfeeds.com/p/mta/79/'\n",
    "\n",
    "# # Create a directory to store the downloaded files, if it doesn't exist\n",
    "# if not os.path.exists('files/zip_files'):\n",
    "#     os.makedirs('files/zip_files')\n",
    "\n",
    "# # Create a directory to store the extracted files, if it doesn't exist\n",
    "# if not os.path.exists('files/extracted'):\n",
    "#     os.makedirs('files/extracted')\n",
    "\n",
    "# # Loop through each date in a_text_list\n",
    "# for date_text in a_text_list:\n",
    "#     # Construct the URL for the current date\n",
    "#     url = f'{base_url}{date_text}/download'\n",
    "\n",
    "#     # Define the filenames and directories for the downloaded and extracted data\n",
    "#     zip_filename = f'files/zip_files/{date_text}.zip'\n",
    "#     extract_dir = f'files/extracted/{date_text}'\n",
    "\n",
    "#     # Check if the ZIP file already exists\n",
    "#     if os.path.exists(zip_filename):\n",
    "#         print(f'{zip_filename} already exists')\n",
    "#     else:\n",
    "#         # Download the data and save it to a file\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             with open(zip_filename, 'wb') as f:\n",
    "#                 f.write(response.content)\n",
    "#             print(f'Successfully downloaded data for {date_text}')\n",
    "#         else:\n",
    "#             print(f'Error downloading data for {date_text}')\n",
    "#             continue\n",
    "\n",
    "#     # Check if the target directory already exists\n",
    "#     if os.path.exists(extract_dir):\n",
    "#         print(f'{extract_dir} already exists')\n",
    "#     else:\n",
    "#         # Extract the contents of the ZIP file to the target directory\n",
    "#         with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "#             zip_ref.extractall(extract_dir)\n",
    "#         print(f'Successfully extracted data for {date_text}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8b887db99f8edfa7f3e3a008cc1b31fa63afeb09acb3c9f070418078094f7b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
