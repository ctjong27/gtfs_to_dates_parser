{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages: 6\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://transitfeeds.com/p/mta/79\"\n",
    "response = requests.get(url)\n",
    "total_pages = 0\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    pagination = soup.find(\"ul\", class_=\"pagination\")\n",
    "    last_page_link = pagination.find_all(\"a\")[-1] # second to last link\n",
    "    total_pages = int(last_page_link.text)\n",
    "    print(f\"Total number of pages: {total_pages}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20221129', '20221121', '20220615', '20211210', '20211109', '20210713', '20210615', '20210503', '20210315', '20201102', '20201001', '20200910', '20200814', '20200430', '20200109', '20191231', '20191112', '20191003', '20190909', '20190509', '20190423', '20190420', '20181221', '20181113', '20181004', '20180908', '20180708', '20180622', '20180615', '20180109', '20171109', '20171106', '20170919', '20170712', '20170619', '20170130', '20170124', '20170113', '20161222', '20161210', '20161107', '20161103', '20160627', '20160502', '20151207', '20150914', '20150909', '20150901', '20150616', '20141204', '20140919', '20140822', '20140801', '20140715', '20140626', '20140326', '20140204', '20131030']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "a_text_list = []\n",
    "\n",
    "# Loop through pages 1 to total_pages\n",
    "for page_num in range(1, total_pages + 1):\n",
    "    url = f'https://transitfeeds.com/p/mta/79?p={page_num}'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the first tbody element\n",
    "    tbody_element = soup.find('tbody')\n",
    "\n",
    "    # Find all tr elements within the current tbody\n",
    "    tr_elements = tbody_element.find_all('tr')\n",
    "\n",
    "    # Loop through each tr element\n",
    "    for tr in tr_elements:\n",
    "        # Find the first td element within the current tr\n",
    "        td_element = tr.find('td')\n",
    "\n",
    "        # Find the first a element within the current td and extract its text\n",
    "        a_element = td_element.find('a')\n",
    "        a_text = a_element.text\n",
    "\n",
    "        # Convert the date string to the desired format\n",
    "        date_obj = datetime.strptime(a_text, '%d %B %Y')\n",
    "        formatted_date = date_obj.strftime('%Y%m%d')\n",
    "        a_text_list.append(formatted_date)\n",
    "\n",
    "print(a_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data for 20221129\n",
      "Successfully extracted data for 20221129\n",
      "Successfully downloaded data for 20221121\n",
      "Successfully extracted data for 20221121\n",
      "Successfully downloaded data for 20220615\n",
      "Successfully extracted data for 20220615\n",
      "Successfully downloaded data for 20211210\n",
      "Successfully extracted data for 20211210\n",
      "Successfully downloaded data for 20211109\n",
      "Successfully extracted data for 20211109\n",
      "Successfully downloaded data for 20210713\n",
      "Successfully extracted data for 20210713\n",
      "Successfully downloaded data for 20210615\n",
      "Successfully extracted data for 20210615\n",
      "Successfully downloaded data for 20210503\n",
      "Successfully extracted data for 20210503\n",
      "Successfully downloaded data for 20210315\n",
      "Successfully extracted data for 20210315\n",
      "Successfully downloaded data for 20201102\n",
      "Successfully extracted data for 20201102\n",
      "Successfully downloaded data for 20201001\n",
      "Successfully extracted data for 20201001\n",
      "Successfully downloaded data for 20200910\n",
      "Successfully extracted data for 20200910\n",
      "Successfully downloaded data for 20200814\n",
      "Successfully extracted data for 20200814\n",
      "Successfully downloaded data for 20200430\n",
      "Successfully extracted data for 20200430\n",
      "Successfully downloaded data for 20200109\n",
      "Successfully extracted data for 20200109\n",
      "Successfully downloaded data for 20191231\n",
      "Successfully extracted data for 20191231\n",
      "Successfully downloaded data for 20191112\n",
      "Successfully extracted data for 20191112\n",
      "Successfully downloaded data for 20191003\n",
      "Successfully extracted data for 20191003\n",
      "Successfully downloaded data for 20190909\n",
      "Successfully extracted data for 20190909\n",
      "Successfully downloaded data for 20190509\n",
      "Successfully extracted data for 20190509\n",
      "Successfully downloaded data for 20190423\n",
      "Successfully extracted data for 20190423\n",
      "Successfully downloaded data for 20190420\n",
      "Successfully extracted data for 20190420\n",
      "Successfully downloaded data for 20181221\n",
      "Successfully extracted data for 20181221\n",
      "Successfully downloaded data for 20181113\n",
      "Successfully extracted data for 20181113\n",
      "Successfully downloaded data for 20181004\n",
      "Successfully extracted data for 20181004\n",
      "Successfully downloaded data for 20180908\n",
      "Successfully extracted data for 20180908\n",
      "Successfully downloaded data for 20180708\n",
      "Successfully extracted data for 20180708\n",
      "Successfully downloaded data for 20180622\n",
      "Successfully extracted data for 20180622\n",
      "Successfully downloaded data for 20180615\n",
      "Successfully extracted data for 20180615\n",
      "Successfully downloaded data for 20180109\n",
      "Successfully extracted data for 20180109\n",
      "Successfully downloaded data for 20171109\n",
      "Successfully extracted data for 20171109\n",
      "Successfully downloaded data for 20171106\n",
      "Successfully extracted data for 20171106\n",
      "Successfully downloaded data for 20170919\n",
      "Successfully extracted data for 20170919\n",
      "Successfully downloaded data for 20170712\n",
      "Successfully extracted data for 20170712\n",
      "Successfully downloaded data for 20170619\n",
      "Successfully extracted data for 20170619\n",
      "Successfully downloaded data for 20170130\n",
      "Successfully extracted data for 20170130\n",
      "Successfully downloaded data for 20170124\n",
      "Successfully extracted data for 20170124\n",
      "Successfully downloaded data for 20170113\n",
      "Successfully extracted data for 20170113\n",
      "Successfully downloaded data for 20161222\n",
      "Successfully extracted data for 20161222\n",
      "Successfully downloaded data for 20161210\n",
      "Successfully extracted data for 20161210\n",
      "Successfully downloaded data for 20161107\n",
      "Successfully extracted data for 20161107\n",
      "Successfully downloaded data for 20161103\n",
      "Successfully extracted data for 20161103\n",
      "Successfully downloaded data for 20160627\n",
      "Successfully extracted data for 20160627\n",
      "Successfully downloaded data for 20160502\n",
      "Successfully extracted data for 20160502\n",
      "Successfully downloaded data for 20151207\n",
      "Successfully extracted data for 20151207\n",
      "Successfully downloaded data for 20150914\n",
      "Successfully extracted data for 20150914\n",
      "Successfully downloaded data for 20150909\n",
      "Successfully extracted data for 20150909\n",
      "Successfully downloaded data for 20150901\n",
      "Successfully extracted data for 20150901\n",
      "Successfully downloaded data for 20150616\n",
      "Successfully extracted data for 20150616\n",
      "Error downloading data for 20141204\n",
      "Error downloading data for 20140919\n",
      "Error downloading data for 20140822\n",
      "Error downloading data for 20140801\n",
      "Error downloading data for 20140715\n",
      "Error downloading data for 20140626\n",
      "Error downloading data for 20140326\n",
      "Error downloading data for 20140204\n",
      "Error downloading data for 20131030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://transitfeeds.com/p/mta/79/'\n",
    "\n",
    "# Create a directory to store the downloaded files, if it doesn't exist\n",
    "if not os.path.exists('files/zip_files'):\n",
    "    os.makedirs('files/zip_files')\n",
    "\n",
    "# Create a directory to store the extracted files, if it doesn't exist\n",
    "if not os.path.exists('files/extracted'):\n",
    "    os.makedirs('files/extracted')\n",
    "\n",
    "# Loop through each date in a_text_list\n",
    "for date_text in a_text_list:\n",
    "    # Construct the URL for the current date\n",
    "    url = f'{base_url}{date_text}/download'\n",
    "\n",
    "    # Define the filenames and directories for the downloaded and extracted data\n",
    "    zip_filename = f'files/zip_files/{date_text}.zip'\n",
    "    extract_dir = f'files/extracted/{date_text}'\n",
    "\n",
    "    # Check if the ZIP file already exists\n",
    "    if os.path.exists(zip_filename):\n",
    "        print(f'{zip_filename} already exists')\n",
    "    else:\n",
    "        # Download the data and save it to a file\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(zip_filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f'Successfully downloaded data for {date_text}')\n",
    "        else:\n",
    "            print(f'Error downloading data for {date_text}')\n",
    "            continue\n",
    "\n",
    "    # Check if the target directory already exists\n",
    "    if os.path.exists(extract_dir):\n",
    "        print(f'{extract_dir} already exists')\n",
    "    else:\n",
    "        # Extract the contents of the ZIP file to the target directory\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "        print(f'Successfully extracted data for {date_text}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8b887db99f8edfa7f3e3a008cc1b31fa63afeb09acb3c9f070418078094f7b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
